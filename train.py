# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H7gVTptzNmcfpkA4-S1hoMdmhbac0JFJ
"""

def train_model(model, train_r, train_m, test_r, test_m, epoch_p, epoch_f, optimizer_p, optimizer_f):
    # Define RMSE loss function
    def rmse_loss(predictions, targets, mask):
        diff = mask * (predictions - targets)
        return torch.sqrt((diff**2).sum() / mask.sum())

    best_ndcg_ep = 0
    best_ndcg = 0
    time_cumulative = 0

    # Pre-training phase
    for epoch in range(epoch_p):

        # Forward pass
        reg_losses, outputs = model(train_r, train_m)

        print(f'outputs:{outputs.shape}')

        # Compute loss (RMSE + regularization)
        loss = rmse_loss(torch.clamp(outputs, 1., 5.), train_r, train_m) + reg_losses

        # Backward pass and optimization
        optimizer_p.zero_grad()
        loss.backward()
        optimizer_p.step()

        print('==========' * 12)
        print('PRE-TRAINING')
        print('Epoch:', epoch + 1)
        print('==========' * 12)

    # Fine-tuning phase
    for epoch in range(epoch_f):

        # Forward pass
        reg_losses, foutputs  = model(train_r, train_m)

        # Compute loss (RMSE + regularization)
        loss = rmse_loss(torch.clamp(foutputs, 1., 5.), train_r, train_m) + reg_losses

        # Backward pass and optimization
        optimizer_f.zero_grad()
        loss.backward()
        optimizer_f.step()

        test_ndcg = call_ndcg(torch.clamp(foutputs, 1., 5.), test_r)

        if best_ndcg < test_ndcg:
            best_ndcg = test_ndcg
            best_ndcg_ep = epoch + 1

        print('==========' * 12)
        print('FINE-TUNING')
        print('Epoch:', epoch + 1, 'test ndcg:', test_ndcg)
        print('==========' * 12)

    return best_ndcg_ep, best_ndcg, foutputs